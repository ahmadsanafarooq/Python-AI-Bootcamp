{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4caee843",
   "metadata": {},
   "source": [
    "!pip install langchain langchain-community langchain-chroma langchain-google-genai python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5f8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb54b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "file_path = r\"E:\\DS Material\\Langchian AI Agents\\4_Rag\\Documents\\lord_of_the_rings.txt\"  # Text file location\n",
    "persistent_directory = \"db/chroma_db\"             # Folder to store embeddings\n",
    "# Make sure the vector store folder exists\n",
    "os.makedirs(persistent_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf58250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "loader = TextLoader(file_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57aa415",
   "metadata": {},
   "source": [
    "**Chunks Overlap**\n",
    "\n",
    "The overlap between the two sets of covers is the set of all covers that are common to both\n",
    "sets. This is the set of all covers that are in both sets of covers.\n",
    "If we set overlap 0 it means that there are no common covers between the two sets of covers.\n",
    "if we set it to 50 it means that 50% of the covers in the first set are common to the second set of covers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4558ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1623, which is longer than the specified 1000\n",
      "Created a chunk of size 1315, which is longer than the specified 1000\n",
      "Created a chunk of size 1058, which is longer than the specified 1000\n",
      "Created a chunk of size 1345, which is longer than the specified 1000\n",
      "Created a chunk of size 1329, which is longer than the specified 1000\n",
      "Created a chunk of size 1997, which is longer than the specified 1000\n",
      "Created a chunk of size 1418, which is longer than the specified 1000\n",
      "Created a chunk of size 1107, which is longer than the specified 1000\n",
      "Created a chunk of size 1200, which is longer than the specified 1000\n",
      "Created a chunk of size 1233, which is longer than the specified 1000\n",
      "Created a chunk of size 1195, which is longer than the specified 1000\n",
      "Created a chunk of size 1055, which is longer than the specified 1000\n",
      "Created a chunk of size 1505, which is longer than the specified 1000\n",
      "Created a chunk of size 1355, which is longer than the specified 1000\n",
      "Created a chunk of size 2073, which is longer than the specified 1000\n",
      "Created a chunk of size 1005, which is longer than the specified 1000\n",
      "Created a chunk of size 1652, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and split into 43 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split the document into chunks\n",
    "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Loaded and split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef5a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create embeddings using Google model\n",
    "load_dotenv()\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922543de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created and saved in Chroma DB!\n"
     ]
    }
   ],
   "source": [
    "# Save to Chroma vector store\n",
    "db = Chroma.from_documents(chunks, embeddings, persist_directory=persistent_directory)\n",
    "\n",
    "print(\"Embeddings created and saved in Chroma DB!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c3a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for chunk 1 (length 768):\n",
      "[-0.0030382543336600065, -0.011926127597689629, -0.025277910754084587, -0.033504847437143326, 0.05894811823964119, 0.007486980874091387, 0.0019972422160208225, -0.043876051902770996, 0.0036799057852476835, 0.02139812894165516]...\n",
      "Vector for chunk 2 (length 768):\n",
      "[0.0024122989270836115, 0.010658379644155502, -0.0338442400097847, -0.02982841432094574, 0.03885631263256073, 0.025831248611211777, -0.03406795114278793, -0.07525389641523361, 0.046095799654722214, -0.003920626826584339]...\n",
      "Vector for chunk 3 (length 768):\n",
      "[0.05259506404399872, 0.0016552802408114076, -0.07177039980888367, 0.0015449881320819259, 0.09196153283119202, 0.026325594633817673, -0.052662208676338196, -0.038795001804828644, 0.046615321189165115, 0.006928024347871542]...\n"
     ]
    }
   ],
   "source": [
    "# Loop through the first 3 document chunks\n",
    "for i, doc in enumerate(chunks[:3]):\n",
    "    # Generate the embedding vector for the text content of the chunk\n",
    "    vector = embeddings.embed_query(doc.page_content)\n",
    "    \n",
    "    # Print the chunk index, vector length, and first 10 dimensions of the embedding\n",
    "    print(f\"Vector for chunk {i+1} (length {len(vector)}):\\n{vector[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba57a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a quiet evening in the Shire, Gandalf came to Frodo in Hobbiton and revealed the terrible truth about the One Ring. They sat in the warmth of Bag End, and Gandalf laid out the importance of Frodoâ€™s role in the fate of Middle-earth\n"
     ]
    }
   ],
   "source": [
    "# Load from existing Chroma DB\n",
    "db = Chroma(\n",
    "    persist_directory=persistent_directory,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Search with a sample question\n",
    "results = db.similarity_search(\"Who is Frodo?\", k=1)\n",
    "for doc in results:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67116a35",
   "metadata": {},
   "source": [
    "# **Part 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "870b867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use current working directory\n",
    "current_dir = os.getcwd()\n",
    "vector_db_folder = os.path.join(current_dir, \"db\", \"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "655c417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "db = Chroma(persist_directory=vector_db_folder, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "747d906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Documents\n",
      "Document 1:\n",
      "Gandalf had been a friend to the Bagginses for many years, and he came to Hobbiton to visit Frodo one summer day. He found him sitting outside Bag End, the home of the Baggins family. It was here that Gandalf first spoke to Frodo about the dangers of the One Ring\n",
      "\n",
      "Frodo was surprised to see Gandalf arriving at his doorstep in Hobbiton, for he had not expected the wizard for some time. The conversation they began was far more serious than any previous meeting, as Gandalf had urgent news regarding the Ring that Frodo had inherited.\n",
      "\n",
      "Source: E:\\DS Material\\Langchian AI Agents\\4_Rag\\Documents\\lord_of_the_rings.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the user's question\n",
    "query = \"Where does Gandalf meet Frodo?\"\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1},\n",
    ")\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Display the relevant results with metadata\n",
    "print(\"Relevant Documents\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
